from __future__ import annotations
import logging
from datetime import datetime, UTC
from typing import List, Optional, Tuple
from uuid import UUID
from fastapi import UploadFile, HTTPException
from sqlmodel import Session, select, func, delete
from app.models import Document, DocumentPage, ExtractedField, User, SystemConfig
from app.services.storage import get_storage, StoredObject
from app.services.extraction_queue import enqueue_extraction_job

logger = logging.getLogger(__name__)

class DocumentService:
    """
    Unified service for document metadata and storage management.
    (Phase 2.1 of the architecture plan)
    """

    def __init__(self, db: Session):
        self.db = db
        self.storage = get_storage()

    def create_document(self, user_id: UUID, file: UploadFile, doc_type: str = "unknown") -> Document:
        """Upload file to storage and create metadata record."""
        doc_id = UUID(int=0) # Temporary, will be generated by DB
        
        # 1. Save to storage
        try:
            stored: StoredObject = self.storage.save_upload(doc_id, file)
        except Exception as e:
            logger.error(f"Storage upload failed: {e}")
            raise HTTPException(status_code=500, detail="File upload failed")

        # 2. Create DB record
        doc = Document(
            user_id=user_id,
            name=file.filename or "unnamed_document",
            file_path=stored.path,
            file_size=stored.size,
            mime_type=file.content_type,
            status="pending",
            type=doc_type,
            created_at=datetime.now(UTC),
            updated_at=datetime.now(UTC)
        )
        self.db.add(doc)
        self.db.commit()
        self.db.refresh(doc)

        # 3. Rename file to include correct ID if needed by storage provider
        # (Already handled by save_upload if it uses document_id)

        # 4. Trigger extraction
        enqueue_extraction_job(session=self.db, document_id=doc.id)
        
        return doc

    def get_document(self, document_id: UUID, user_id: Optional[UUID] = None) -> Document:
        """Fetch document details."""
        query = select(Document).where(Document.id == document_id)
        if user_id:
            query = query.where(Document.user_id == user_id)
        
        doc = self.db.exec(query).first()
        if not doc:
            raise HTTPException(status_code=404, detail="Document not found")
        return doc

    def get_document_with_details(self, document_id: UUID, user_id: UUID) -> Tuple[Document, List[ExtractedField], List[DocumentPage]]:
        """Fetch document with related fields and pages."""
        doc = self.get_document(document_id, user_id)
        
        fields = self.db.exec(
            select(ExtractedField)
            .where(ExtractedField.document_id == doc.id)
            .order_by(ExtractedField.page_number.asc(), ExtractedField.created_at.asc())
        ).all()
        
        pages = self.db.exec(
            select(DocumentPage)
            .where(DocumentPage.document_id == doc.id)
            .order_by(DocumentPage.page_number.asc())
        ).all()
        
        return doc, list(fields), list(pages)

    def render_pages(self, document_id: UUID, user_id: UUID) -> List[DocumentPage]:
        """Convert PDF to images and save page records."""
        doc = self.get_document(document_id, user_id)
        
        if (doc.mime_type or "").lower() != "application/pdf" and not doc.file_path.lower().endswith(".pdf"):
            raise HTTPException(status_code=400, detail="Not a PDF")

        from app.services.pdf_render import render_pdf_to_png_pages
        pdf_bytes = self.storage.read_bytes(doc.file_path)
        rendered = render_pdf_to_png_pages(pdf_bytes)

        # 1. Clear existing
        self.db.exec(delete(DocumentPage).where(DocumentPage.document_id == doc.id))

        # 2. Save new
        pages = []
        for p in rendered:
            rel = f"pages/{doc.id}/{p.page_number}.png"
            self.storage.save_bytes(rel, p.png_bytes)
            
            page_obj = DocumentPage(
                document_id=doc.id,
                page_number=p.page_number,
                image_path=rel,
                width=p.width,
                height=p.height,
                created_at=datetime.now(UTC)
            )
            self.db.add(page_obj)
            pages.append(page_obj)

        doc.pages = len(rendered)
        doc.updated_at = datetime.now(UTC)
        self.db.add(doc)
        self.db.commit()
        
        return pages

    def get_page_image(self, document_id: UUID, page_number: int, user_id: UUID):
        """Build storage response for a page image."""
        doc = self.get_document(document_id, user_id)
        page = self.db.exec(
            select(DocumentPage).where(
                DocumentPage.document_id == doc.id,
                DocumentPage.page_number == page_number
            )
        ).first()
        
        if not page or not page.image_path:
            raise HTTPException(status_code=404, detail="Page image not found")
        
        return self.storage.build_file_response(page.image_path, media_type="image/png")

    def list_documents(
        self, 
        user_id: UUID, 
        limit: int = 50, 
        offset: int = 0,
        q: Optional[str] = None,
        status: Optional[str] = None,
        type: Optional[str] = None,
        created_from: Optional[datetime] = None,
        created_to: Optional[datetime] = None
    ) -> Tuple[List[Document], int]:
        """List documents with pagination and advanced filters."""
        query = select(Document).where(Document.user_id == user_id)
        
        if status:
            query = query.where(Document.status == status)
        if type:
            query = query.where(Document.type == type)
        if q:
            query = query.where(Document.name.ilike(f"%{q}%"))
        if created_from:
            query = query.where(Document.created_at >= created_from)
        if created_to:
            query = query.where(Document.created_at <= created_to)
        
        total = self.db.exec(select(func.count()).select_from(query.subquery())).one()
        results = self.db.exec(query.order_by(Document.created_at.desc()).offset(offset).limit(limit)).all()
        
        return list(results), total

    def delete_document(self, document_id: UUID, user_id: UUID) -> dict:
        """Delete document record and associated storage/pages/fields."""
        doc = self.get_document(document_id, user_id)
        doc_meta = {"name": doc.name, "type": doc.type, "status": doc.status}
        
        # 1. Delete associated data
        self.db.exec(delete(DocumentPage).where(DocumentPage.document_id == document_id))
        self.db.exec(delete(ExtractedField).where(ExtractedField.document_id == document_id))
        
        # 2. Delete files from storage
        try:
            self.storage.delete(doc.file_path)
            # Delete pages folder if possible (depends on provider implementation)
            for page_path in self.storage.list_prefix(f"pages/{document_id}"):
                self.storage.delete(page_path)
        except Exception as e:
            logger.warning(f"Failed to delete storage for {document_id}: {e}")

        # 3. Delete record
        self.db.delete(doc)
        self.db.commit()
        return doc_meta

    def get_file_response(self, document_id: UUID, user_id: UUID):
        """Build storage response for a document file."""
        doc = self.get_document(document_id, user_id)
        return self.storage.build_file_response(doc.file_path, media_type=doc.mime_type, filename=doc.name)

    def export_documents_zip(self, document_ids: List[UUID], user_id: UUID) -> Tuple[BytesIO, str, int, int]:
        """Export multiple documents as a ZIP archive."""
        from zipfile import ZIP_DEFLATED, ZipFile
        from io import BytesIO
        from pathlib import PurePosixPath
        
        docs = self.db.exec(
            select(Document).where(Document.user_id == user_id, Document.id.in_(document_ids))
        ).all()
        
        if not docs:
            raise HTTPException(status_code=404, detail="No documents found")

        requested_count = len(document_ids)
        exported_count = 0
        buffer = BytesIO()
        
        with ZipFile(buffer, mode="w", compression=ZIP_DEFLATED) as zipf:
            for doc in docs:
                try:
                    payload = self.storage.read_bytes(doc.file_path)
                except Exception:
                    continue

                if not payload:
                    continue

                ext = PurePosixPath(doc.file_path).suffix
                safe_name = (doc.name or str(doc.id)).strip() or str(doc.id)
                safe_name = safe_name.replace("/", "_").replace("\\", "_")
                
                if ext and safe_name.lower().endswith(ext.lower()):
                    arc_name = f"{doc.id}_{safe_name}"
                else:
                    arc_name = f"{doc.id}_{safe_name}{ext}"
                
                try:
                    zipf.writestr(arc_name, payload)
                    exported_count += 1
                except OSError:
                    continue

        if buffer.getbuffer().nbytes == 0:
            raise HTTPException(status_code=404, detail="No readable files for selected documents")

        buffer.seek(0)
        filename = f"document-export-{datetime.now(UTC).strftime('%Y%m%d-%H%M%S')}.zip"
        return buffer, filename, requested_count, exported_count

    def search_advanced(self, user_id: UUID, query: str, limit: int = 50) -> List[Document]:
        """
        Perform advanced full-text search across documents (Phase 3.3).
        Uses PostgreSQL FTS features if available, falls back to ILIKE.
        """
        if not query:
            return []

        from sqlalchemy import text
        try:
            # Attempt PostgreSQL Full-Text Search
            sql = text("""
                SELECT id FROM documents 
                WHERE user_id = :user_id AND status = 'completed'
                AND (
                    to_tsvector('english', COALESCE(full_text, '') || ' ' || COALESCE(name, '')) @@ plainto_tsquery('english', :query)
                    OR name ILIKE :ilike_query
                )
                ORDER BY ts_rank(to_tsvector('english', COALESCE(full_text, '') || ' ' || COALESCE(name, '')), plainto_tsquery('english', :query)) DESC
                LIMIT :limit
            """)
            results = self.db.execute(sql, {
                "user_id": user_id, 
                "query": query, 
                "ilike_query": f"%{query}%", 
                "limit": limit
            }).all()
            doc_ids = [r[0] for r in results]
            
            if not doc_ids:
                return []
            
            # Fetch full objects preserving order
            docs = self.db.exec(select(Document).where(Document.id.in_(doc_ids))).all()
            doc_map = {d.id: d for d in docs}
            return [doc_map[tid] for tid in doc_ids if tid in doc_map]
            
        except Exception as e:
            logger.warning(f"Advanced search failed (likely non-Postgres), falling back: {e}")
            return self.db.exec(
                select(Document)
                .where(Document.user_id == user_id)
                .where((Document.full_text.ilike(f"%{query}%")) | (Document.name.ilike(f"%{query}%")))
                .limit(limit)
            ).all()

    def get_similar_documents(self, document_id: UUID, user_id: UUID, limit: int = 5) -> List[Document]:
        """
        Discover similar documents based on shared extracted fields (Phase 3.3).
        """
        doc = self.get_document(document_id, user_id)
        
        # Simple similarity based on same vendor/type or overlapping field names/values
        # In the future, this should use vector embeddings
        query = select(Document).where(Document.user_id == user_id, Document.id != document_id)
        
        if doc.type and doc.type != "unknown":
            query = query.where(Document.type == doc.type)
            
        # Try to find vendor from extracted fields
        vendor_field = self.db.exec(
            select(ExtractedField)
            .where(ExtractedField.document_id == document_id)
            .where(ExtractedField.field_name.ilike("%vendor%"))
        ).first()
        
        if vendor_field and vendor_field.field_value:
            # Find documents with same vendor
            vendor_val = vendor_field.field_value
            query = query.join(ExtractedField, Document.id == ExtractedField.document_id)
            query = query.where(ExtractedField.field_name.ilike("%vendor%"), ExtractedField.field_value == vendor_val)
            
        return self.db.exec(query.order_by(Document.created_at.desc()).limit(limit)).all()

    async def generate_summary(self, document_id: UUID, user_id: UUID) -> str:
        """
        Generate an AI summary of document content (Phase 3.3).
        """
        doc = self.get_document(document_id, user_id)
        
        # Return existing summary if available
        if doc.ai_summary:
            return doc.ai_summary
            
        if not doc.full_text:
            raise HTTPException(status_code=400, detail="Document has no text content to summarize")
            
        from app.services.llm_service import extract_fields_custom
        
        prompt = (
            "Summarize the following document content in 2-3 concise sentences. "
            "Focus on the main purpose, key parties, and any critical dates or amounts.\n\n"
            f"TEXT:\n{doc.full_text[:8000]}"
        )
        
        try:
            # Using extract_fields_custom which returns LlmResult
            result = await extract_fields_custom(doc.full_text[:8000], prompt)
            
            # Extract summary string from result
            summary_text = "Summary not available"
            if isinstance(result.data, dict):
                summary_text = result.data.get("summary", str(result.data))
            elif isinstance(result.data, str):
                summary_text = result.data
                
            # Save back to document
            doc.ai_summary = summary_text
            self.db.add(doc)
            self.db.commit()
            self.db.refresh(doc)
            
            return summary_text
        except Exception as e:
            logger.error(f"Summarization failed: {e}")
            return "Failed to generate summary"
